{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import re\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Embedding\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/Train.txt'\n",
    "dic_file = 'data/dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0\\t财经\\t上证50ETF净申购突增', '0\\t财经\\t交银施罗德保本基金将发行', '0\\t财经\\t基金公司不裁员反扩军 走访名校揽人才', '0\\t财经\\t基金巨亏30亿 欲打开云天系跌停自救', '0\\t财经\\t基金市场周二缩量走低']\n"
     ]
    }
   ],
   "source": [
    "# 加载并查看数据\n",
    "with open(train_file,'r',encoding='utf-8') as f:\n",
    "    text = f.read().split('\\n')\n",
    "print(text[:5])\n",
    "# 经过查看，测试文件中的一行代表一个样本，一条样本数据由三个部分组成，第一部分是类别对应的编码，第二部分是类别对应的中文名称，第三部分是样本的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典文件是json格式，这种格式的数据由键值对组成，其中，第一个称为键，第二个称为值，在本次实验中，键表示一个字，值表示这个字的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载字典\n",
    "word2id_dic = dict()\n",
    "with open(dic_file,'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    word2id_dic = eval(text)\n",
    "# 获取未编码数字id\n",
    "unknown_word_id = word2id_dic['<unk>']\n",
    "word2id_dic['<pad>'] = len(word2id_dic) # 表示非定长数据的填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行数据处理\n",
    "# 该处理过程首先读取datafile中的文件，并利用dic字典，将其中的文本转为one-hot编码格式，同时，处理对应的label标签\n",
    "train = []\n",
    "classes = dict()\n",
    "max_seq_len = 0\n",
    "with open(train_file,'r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line[:-1]\n",
    "        sample = line.split('\\t')\n",
    "        sample[0] = int(sample[0])\n",
    "        max_seq_len = max_seq_len if max_seq_len >= len(sample[2]) else len(sample[2])\n",
    "        train.append(sample)\n",
    "        if sample[0] not in classes:\n",
    "            classes[sample[0]] = sample[1]\n",
    "        elif sample[1] != classes[sample[0]]:\n",
    "            raise Exception('{}类别对应到了不同的标签，请检查数据'.format(sample[1]))\n",
    "train_set = []\n",
    "for sample in train:\n",
    "    encode_data = []\n",
    "    for c in sample[2]:\n",
    "        encode_data.append(word2id_dic.get(c,unknown_word_id))\n",
    "    train_set.append((encode_data,int(sample[0])))\n",
    "np.random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总量 752476\n"
     ]
    }
   ],
   "source": [
    "print('样本总量',len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python高级程序设计\\.venv\\lib\\site-packages\\matplotlib_inline\\config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n",
      "d:\\python高级程序设计\\.venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), array([ 33389,   6830,  18045, 138959,  29328,  37743, 146637,  45765,\n",
      "        12032,  56778, 118444,   3221,  21936,  83369], dtype=int64))\n",
      "科技 146637\n",
      "星座 3221\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "labels = []\n",
    "for i in train_set:\n",
    "    labels.append(i[1])\n",
    "sns.countplot(labels)\n",
    "labels = np.array(labels).astype(np.int32)\n",
    "labels = np.unique(labels,return_counts=True)\n",
    "print(labels)\n",
    "# 数据不平衡的情况比较严重\n",
    "# 返回值最大、最小的索引位置\n",
    "max = np.argmax(labels[1])\n",
    "min = np.argmin(labels[1])\n",
    "print(classes[max],labels[1][max]) #\n",
    "print(classes[min],labels[1][min])\n",
    "# 数据不平衡比较严重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "\n",
    "x = [[x] for x in x]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写迭代器，每次按照batch_size的大小，返回样本量\n",
    "def build_batch(data_set,batch_size = 128,max_seq_len = 128,shuffle = True):\n",
    "    \n",
    "    sentence_batch = [] # 代表一个mini-batch的句子\n",
    "    sentence_label_batch = [] # 表示每个句子对应的分类\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data_set)\n",
    "\n",
    "    for sentence,label in data_set:\n",
    "        sample = sentence[:]\n",
    "        if len(sentence) < max_seq_len:\n",
    "            for _ in range(max_seq_len-len(sample)):\n",
    "                sample.append(word2id_dic['<pad>'])\n",
    "        \n",
    "        sample = [[word] for word in sample]\n",
    "        sentence_batch.append(sample)\n",
    "        sentence_label_batch.append([label])\n",
    "\n",
    "        if len(sentence_batch) == batch_size:\n",
    "            yield np.array(sentence_batch).astype(np.int64),np.array(sentence_label_batch).astype(np.int64)\n",
    "            sentence_label_batch = []\n",
    "            sentence_batch = []\n",
    "    if len(sentence_batch) >= 0 and len(sentence_batch) <= batch_size:\n",
    "        yield np.array(sentence_batch).astype(np.int64),np.array(sentence_label_batch).astype(np.int64)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个用于情感分类的网络实例，SentimentClassifier\n",
    "class SentimentClassifier(paddle.nn.Layer):\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, class_num=2, num_steps=128, num_layers=1, init_scale=0.1, dropout_rate=None):\n",
    "        \n",
    "        # 参数含义如下：\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\n",
    "        # 3.embedding_size，表示词向量的维度\n",
    "        # 4.class_num，情感类型个数，可以是2分类，也可以是多分类\n",
    "        # 5.num_steps，表示这个情感分析模型最大可以考虑的句子长度\n",
    "        # 6.num_layers，表示网络的层数\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围,长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，\\\n",
    "        # 这些函数对数值精度非常敏感，因此我们一般只使用比较小的初始化范围，以保证效果\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.class_num = class_num\n",
    "        self.num_steps = num_steps\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.init_scale = init_scale\n",
    "       \n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\n",
    "        \n",
    "        # 声明使用上述语义向量映射到具体情感类别时所需要使用的线性层\n",
    "        self.cls_fc = paddle.nn.Linear(in_features=self.hidden_size, out_features=self.class_num, \n",
    "                             weight_attr=None, bias_attr=None)\n",
    "        \n",
    "        # 一般在获取单词的embedding后，会使用dropout层，防止过拟合，提升模型泛化能力\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\n",
    "\n",
    "    # forwad函数即为模型前向计算的函数，它有两个输入，分别为：\n",
    "    # input为输入的训练文本，其shape为[batch_size, max_seq_len]\n",
    "    # label训练文本对应的情感标签，其shape维[batch_size, 1]\n",
    "    def forward(self, inputs):\n",
    "        # 获取输入数据的batch_size\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # 本实验默认使用1层的LSTM，首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\n",
    "        init_hidden_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\n",
    "\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量，并且设置stop_gradient=True，避免这些向量被更新，从而影响训练效果\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        # 对应以上第2步，将输入的句子的mini-batch转换为词向量表示，转换后输入数据shape为[batch_size, max_seq_len, embedding_size]\n",
    "        x_emb = self.embedding(inputs)\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])\n",
    "        # 在获取的词向量后添加dropout层\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\n",
    "            x_emb = self.dropout_layer(x_emb)\n",
    "        \n",
    "        # 对应以上第3步，使用LSTM网络，把每个句子转换为语义向量\n",
    "        # 返回的last_hidden即为最后一个时间步的输出，其shape为[self.num_layers, batch_size, hidden_size]\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))\n",
    "        # 提取最后一层隐状态作为文本的语义向量，其shape为[batch_size, hidden_size]\n",
    "        last_hidden = paddle.reshape(last_hidden[-1], shape=[-1, self.hidden_size])\n",
    "\n",
    "        # 对应以上第4步，将每个句子的向量表示映射到具体的情感类别上, logits的维度为[batch_size, 2]\n",
    "        logits = self.cls_fc(last_hidden)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 2.627\n",
      "step 10, loss 2.376\n",
      "step 20, loss 2.339\n",
      "step 30, loss 2.204\n",
      "step 40, loss 2.280\n",
      "step 50, loss 2.312\n",
      "step 60, loss 2.297\n",
      "step 70, loss 2.522\n",
      "step 80, loss 2.418\n",
      "step 90, loss 2.175\n",
      "step 100, loss 2.272\n",
      "step 110, loss 2.274\n",
      "step 120, loss 2.290\n",
      "step 130, loss 2.345\n",
      "step 140, loss 2.210\n",
      "step 150, loss 2.163\n",
      "step 160, loss 2.255\n",
      "step 170, loss 2.152\n",
      "step 180, loss 2.267\n",
      "step 190, loss 2.348\n",
      "step 200, loss 2.269\n",
      "step 210, loss 2.241\n",
      "step 220, loss 2.234\n",
      "step 230, loss 2.200\n",
      "step 240, loss 2.242\n",
      "step 250, loss 2.226\n",
      "step 260, loss 2.364\n",
      "step 270, loss 2.274\n",
      "step 280, loss 2.329\n",
      "step 290, loss 2.254\n",
      "step 300, loss 2.313\n",
      "step 310, loss 2.242\n",
      "step 320, loss 2.211\n",
      "step 330, loss 2.323\n",
      "step 340, loss 2.382\n",
      "step 350, loss 2.292\n",
      "step 360, loss 2.272\n",
      "step 370, loss 2.232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13812/1382225018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m#训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# 保存模型，包含两部分：模型参数和优化器参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13812/1382225018.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# 后向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;31m# 更新参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\paddle\\fluid\\wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\paddle\\fluid\\framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m         assert in_dygraph_mode(\n\u001b[0;32m    226\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\paddle\\fluid\\dygraph\\varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[0;32m    236\u001b[0m                                           framework._dygraph_tracer())\n\u001b[0;32m    237\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m                 core.dygraph_run_backward([self], [grad_tensor], retain_graph,\n\u001b[0m\u001b[0;32m    239\u001b[0m                                           framework._dygraph_tracer())\n\u001b[0;32m    240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "epoch_num = 5\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.2\n",
    "num_layers = 1\n",
    "hidden_size = 256\n",
    "embedding_size = 256\n",
    "max_seq_len = max_seq_len\n",
    "vocab_size = len(word2id_dic)\n",
    "\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    print('use_gpu')\n",
    "    paddle.set_device('gpu:0')\n",
    "\n",
    "# 实例化模型\n",
    "sentiment_classifier = SentimentClassifier(hidden_size, vocab_size, embedding_size,class_num=14,  num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# 指定优化策略，更新模型参数\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= sentiment_classifier.parameters()) \n",
    "\n",
    "# 定义训练函数\n",
    "# 记录训练过程中的损失变化情况，可用于后续画图查看训练情况\n",
    "losses = []\n",
    "steps = []\n",
    "\n",
    "def train(model):\n",
    "    # 开启模型训练模式\n",
    "    model.train()\n",
    "\n",
    "    for epoch_id in range(epoch_num):\n",
    "        # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的情感标签\n",
    "        train_loader = build_batch(train_set, batch_size, max_seq_len)\n",
    "        \n",
    "        for step, (sentences, labels) in enumerate(train_loader):\n",
    "            # 获取数据，并将张量转换为Tensor类型\n",
    "            sentences = paddle.to_tensor(sentences)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            # 前向计算，将数据feed进模型，并得到预测的情感标签和损失\n",
    "            logits = model(sentences)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(input=logits, label=labels, soft_label=False)\n",
    "            loss = paddle.mean(loss)\n",
    "\n",
    "            # 后向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 清除梯度\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                # 记录当前步骤的loss变化情况\n",
    "                losses.append(loss.numpy()[0])\n",
    "                steps.append(step)\n",
    "                # 打印当前loss数值\n",
    "                print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "#训练模型\n",
    "train(sentiment_classifier)\n",
    "\n",
    "# 保存模型，包含两部分：模型参数和优化器参数\n",
    "model_name = \"sentiment_classifier\"\n",
    "# 保存训练好的模型参数\n",
    "paddle.save(sentiment_classifier.state_dict(), \"{}.pdparams\".format(model_name))\n",
    "# 保存优化器参数，方便后续模型继续训练\n",
    "paddle.save(optimizer.state_dict(), \"{}.pdopt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(paddle.nn.Layer):\n",
    "    def __init__(self,hidden_size, vocab_size, embedding_size, class_num=14, num_steps=128, num_layers=1, init_scale=0.1, dropout_rate=None):\n",
    "        super(NewsClassifier,self).__init__()\n",
    "\n",
    "        # 有哪些参数??\n",
    "        # 1.词向量的行-也就是单词的个数\n",
    "        # 2.词向量的列-也就是词向量的维度\n",
    "        # 3.lstm输入层、cell的维度\n",
    "\n",
    "        # 参数含义如下：\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度,同时也是输入的维度\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\n",
    "        # 3.embedding_size，表示词向量的维度\n",
    "        # 4.class_num，新闻分类的个数，可以是2分类，也可以是多分类\n",
    "        # 5.num_steps，表示这个新闻分类的模型最大可以考虑的句子长度\n",
    "        # 6.num_layers，表示网络的层数\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围,长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，\\\n",
    "        # 这些函数对数值精度非常敏感，因此我们一般只使用比较小的初始化范围，以保证效果\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.class_num = class_num\n",
    "        self.num_steps = num_steps\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.init_scale = init_scale\n",
    "\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\n",
    "        # num_embeddings 定义了词表的大小，也就是字典的大小\n",
    "        # embedding_dim 定义了词向量的维度\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\n",
    "        \n",
    "        # 一般在获取单词的embedding后，会使用dropout层，防止过拟合，提升模型泛化能力\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\n",
    "        \n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\n",
    "        # hidden_size 表示隐层的数量，也就是权重矩阵的行；那么，权重矩阵的列数量必须为hidden_size，否则无法进行矩阵运算\n",
    "        # input_size 表示输入向量的维度，需要和embedding层的向量维度相同\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        \n",
    "        # 将LSTM最后一次输出作为全连接的输入\n",
    "        # 声明使用上述语义向量映射到具体情感类别时所需要使用的线性层\n",
    "        self.cls_fc = paddle.nn.Linear(in_features=self.hidden_size, out_features=self.class_num, \n",
    "                             weight_attr=None, bias_attr=None)\n",
    "\n",
    "\n",
    "\n",
    "    def foward(self,inputs,shape = False):\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        # 初始化隐层和cell的参数，初始化为0，表示空白的记忆\n",
    "        # 本实验默认使用1层的LSTM，首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\n",
    "        init_hidden_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32') # num_layers表示LSTM有多少层，batch_size表示有多少个隐层，hidden_size表示隐层的维度\n",
    "        init_cell_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\n",
    "\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量，并且设置stop_gradient=True，避免这些向量被更新，从而影响训练效果\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        # 将输入的句子的mini-batch转换为词向量表示，转换后输入数据shape为[batch_size, max_seq_len, embedding_size]\n",
    "        # 一个单词一个行\n",
    "        x_emb = self.embedding(inputs)\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size]) # 如果不是定长的话，也可以shape = [batch_size,-1,self.embedding_size]\n",
    "        \n",
    "        # 在获取的词向量后添加dropout层\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\n",
    "            x_emb = self.dropout_layer(x_emb)\n",
    "        \n",
    "        # 使用LSTM网络，把每个句子转换为语义向量\n",
    "        # 返回的last_hidden即为最后一个时间步的输出，其shape为[self.num_layers, batch_size, hidden_size]\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell)) # 表示输入、init_hidden、init_cell\n",
    "        \n",
    "        # 提取最后一层隐状态作为文本的语义向量，其shape为[batch_size, hidden_size]\n",
    "        last_hidden = paddle.reshape(last_hidden[-1], shape=[-1, self.hidden_size])\n",
    "\n",
    "        # 对应以上第4步，将每个句子的向量表示映射到具体的情感类别上, logits的维度为[batch_size, 2]\n",
    "        logits = self.cls_fc(last_hidden)\n",
    "\n",
    "        if shape:\n",
    "            print(self.embedding.shape)\n",
    "            print(self.simple_lstm_rnn.shape)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2916/939605304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m# 保存模型，包含两部分：模型参数和优化器参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2916/939605304.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# 前向计算，将数据feed进模型，并得到预测的情感标签和损失\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ddd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\paddle\\fluid\\dygraph\\layers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python高级程序设计\\.venv\\lib\\site-packages\\paddle\\fluid\\dygraph\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0munpacked\u001b[0m \u001b[0mdict\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m         \"\"\"\n\u001b[1;32m--> 920\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "epoch_num = 5\n",
    "# 批大小\n",
    "batch_size = 128\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.01\n",
    "# drouout层随机丢去的权重\n",
    "dropout_rate = 0.2\n",
    "# 设置lstm的层数，这里先设置为1\n",
    "num_layers = 1\n",
    "# 这是隐层的大小\n",
    "hidden_size = 256\n",
    "embedding_size = 256\n",
    "max_seq_len = max_seq_len\n",
    "vocab_size = len(word2id_dic)\n",
    "\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    paddle.set_device('gpu:0')\n",
    "\n",
    "# 实例化模型\n",
    "sentiment_classifier = NewsClassifier(hidden_size, vocab_size, embedding_size,  num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# 指定优化策略，更新模型参数\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= sentiment_classifier.parameters()) \n",
    "\n",
    "# 定义训练函数\n",
    "# 记录训练过程中的损失变化情况，可用于后续画图查看训练情况\n",
    "losses = []\n",
    "steps = []\n",
    "\n",
    "def train(model):\n",
    "    # 开启模型训练模式\n",
    "    model.train()\n",
    "    \n",
    "    # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的情感标签\n",
    "    \n",
    "    \n",
    "    for epoch_id in range(epoch_num):\n",
    "        train_loader = build_batch(train_set, batch_size, max_seq_len)\n",
    "        for step, (sentences, labels) in enumerate(train_loader):\n",
    "            # 获取数据，并将张量转换为Tensor类型\n",
    "            sentences = paddle.to_tensor(sentences)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            # 前向计算，将数据feed进模型，并得到预测的情感标签和损失\n",
    "            logits = model(sentences)\n",
    "\n",
    "            print('ddd')\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(input=logits, label=labels, soft_label=False)\n",
    "            loss = paddle.mean(loss)\n",
    "\n",
    "            # 后向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 清除梯度\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                # 记录当前步骤的loss变化情况\n",
    "                losses.append(loss.numpy()[0])\n",
    "                steps.append(step)\n",
    "                # 打印当前loss数值\n",
    "                print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "#训练模型\n",
    "train(sentiment_classifier)\n",
    "\n",
    "# 保存模型，包含两部分：模型参数和优化器参数\n",
    "model_name = \"sentiment_classifier\"\n",
    "# 保存训练好的模型参数\n",
    "paddle.save(sentiment_classifier.state_dict(), \"{}.pdparams\".format(model_name))\n",
    "# 保存优化器参数，方便后续模型继续训练\n",
    "paddle.save(optimizer.state_dict(), \"{}.pdopt\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73fe03891eabd6446a090ee4c22d8dcac73542d36d6aee5d53443fd4b857a5ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
